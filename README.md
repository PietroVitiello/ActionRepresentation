# A New Representation for Actions in Visual Robot Learning
The aim of this repo is the exploration of new action representations for robot learning.

The main code of this project resides in the src folder. More specifically this repo can be used to generate grasping demonstrations and training or testing models through imitation learning.

<ins>Within the source folder, the most important files are</ins>: 
 * train.py can be run to choose the model configuration and train it
 * test.py can be used to select a model architecture and it will test it on the desired task
 * get_demos.py can be used to generate demonstrations for a given task and scenario (like selecting a scene with or without distractors).
 #
 * The Demos folder has the code necessary to produce the demonstartions as well as the coppeliasim simulations used
 * The Robotics folder has all the kinematics code. This is used to represent the robot as a python class and define the functions used to move it
 * The Learning folder has the entire code used to train the models as well as the various network architectures used in this project

# The Motion Image
This work introduces the idea of using motion images as action representation. These are generated by subtracting together following video frames. As a result, to leverage this idea we propose the MI-Net which makes use of an autoencoder and attention mechanism. The code for these models can be found in src/Learning/Models/MotionIMG
